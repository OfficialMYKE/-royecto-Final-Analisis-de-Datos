{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f068565-1087-49f1-9206-0c8f195d939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11b82ed-3994-43d7-affc-6556eb8aaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventos deportivos\n",
    "\n",
    "url = \"https://www.topendsports.com/events/calendar-2025.php\"\n",
    "resp = requests.get(url)  # Realizar una solicitud HTTP para obtener contenido HTML de la pagina\n",
    "resp.raise_for_status()  # Verifica si la solicitud fue exitosa.\n",
    "soup = BeautifulSoup(resp.content, \"html.parser\") # Analiza el contenido HTML utilizando BeautifulSoup \n",
    "# lo convierte en un objeto navegable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "318f063c-22dc-4a81-86af-c9f53e6cac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.find(\"table\")  # Localiza la primera tabla HTML en la página\n",
    "rows = table.find_all(\"tr\")[1:]  # : Extrae todas las filas (<tr>) menos la primera\n",
    "\n",
    "datos = []\n",
    "for tr in rows:   # Recorre cada fila, extrae el texto dentro de cada columna (<td>)\n",
    "    cols = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "    if len(cols) >= 4:  # Asegura que hay al menos 4 columnas. Si hay menos, se descarta la fila.\n",
    "        date_str, deporte, evento, lugar = cols[:4] # Construye un diccionario por fila y lo agrega a la lista \"datos\"\n",
    "        datos.append({\n",
    "            \"Fecha\": date_str,\n",
    "            \"Deporte\": deporte,\n",
    "            \"Evento\": evento,\n",
    "            \"Ubicación\": lugar\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ecde49d-f23b-45e3-8564-9bb9e62251a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado en eventos_2025.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(datos)  # Convierte la lista de diccionarios en un DataFrame de pandas\n",
    "df.to_csv(\"eventos_2025.csv\", index=False, encoding=\"utf‑8‑sig\") # Exportar el DF a un archivo csv\n",
    "print(\"Guardado en eventos_2025.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "95a9c340-9b7b-4f19-b47e-0af1d9c8147c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62 eventos guardados en eventos_2025_sportstoday.json\n"
     ]
    }
   ],
   "source": [
    "# Eventos deportivos\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://thesportstoday.com/2025-sporting-events-calendar/\"\n",
    "\n",
    "# Obtener contenido de la página\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status()\n",
    "\n",
    "# Crear objeto BeautifulSoup para analizar el HTML\n",
    "soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "\n",
    "# Encontrar la tabla que contiene los eventos\n",
    "table = soup.find(\"table\")\n",
    "if not table:\n",
    "    raise RuntimeError(\"No se encontró la tabla de eventos\")\n",
    "\n",
    "# Obtener todas las filas de la tabla, excluyendo encabezado\n",
    "rows = table.find_all(\"tr\")[1:]  # omitimos encabezado\n",
    "\n",
    "# Lista para almacenar los datos extraídos\n",
    "eventos = []\n",
    "for tr in rows:\n",
    "    cols = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "    if len(cols) >= 4:\n",
    "        fecha, deporte, evento, ubicacion = cols[:4]\n",
    "        eventos.append({\n",
    "            \"fecha\": fecha,\n",
    "            \"deporte\": deporte,\n",
    "            \"evento\": evento,\n",
    "            \"ubicacion\": ubicacion\n",
    "        })\n",
    "\n",
    "# Guardar como JSON\n",
    "with open(\"eventos_2025_sportstoday.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eventos, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"{len(eventos)} eventos guardados en eventos_2025_sportstoday.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14cff1aa-658b-4ec3-bf2b-e3b3eb7cc8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 eventos guardados en eventos_mayo_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# Eventos deportivos\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://sportlaunches.com/2025-sport-events/\"\n",
    "resp = requests.get(url)\n",
    "resp.raise_for_status()\n",
    "soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "\n",
    "# Buscar la sección \"Mai\" que contiene los eventos de mayo\n",
    "section = soup.find(\"h2\", string=lambda t: t and \"Mai\" in t)\n",
    "\n",
    "# Tomamos la tabla siguiente a ese encabezado:\n",
    "table = section.find_next(\"table\") if section else soup.find(\"table\")\n",
    "\n",
    "# Lista para almacenar eventos filtrados por mes\n",
    "eventos = []\n",
    "if table:\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        cols = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "        if len(cols) >= 4:\n",
    "            fecha, nombre, deporte, ubicacion = cols[0], cols[1], cols[2], cols[3]\n",
    "            # Filtrar solo eventos que pertenezcan a mayo (May en inglés o Mai en alemán)\n",
    "            if \"May\" in fecha or \"Mai\" in fecha:\n",
    "                eventos.append({\n",
    "                    \"Fecha\": fecha,\n",
    "                    \"Deporte\": deporte,\n",
    "                    \"Evento\": nombre,\n",
    "                    \"Ubicación\": ubicacion\n",
    "                })\n",
    "                \n",
    "# Exportar los datos a un CSV\n",
    "df = pd.DataFrame(eventos)\n",
    "df.to_csv(\"eventos_mayo_2025.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"{len(df)} eventos guardados en eventos_mayo_2025.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66b6a8bb-991a-471e-b4aa-af5ed5456dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectar con MongoDB\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# URI local o de Atlas (ajústalo según tu caso)\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")  \n",
    "\n",
    "# Conectar a la db de MongoCompass\n",
    "db = client[\"eventos_deportivos_2025\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cf819ea5-f327-4eb4-a47d-afbb961bf67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertManyResult([ObjectId('688e8dbbb4debce916a7b868'), ObjectId('688e8dbbb4debce916a7b869'), ObjectId('688e8dbbb4debce916a7b86a'), ObjectId('688e8dbbb4debce916a7b86b'), ObjectId('688e8dbbb4debce916a7b86c'), ObjectId('688e8dbbb4debce916a7b86d'), ObjectId('688e8dbbb4debce916a7b86e'), ObjectId('688e8dbbb4debce916a7b86f'), ObjectId('688e8dbbb4debce916a7b870'), ObjectId('688e8dbbb4debce916a7b871'), ObjectId('688e8dbbb4debce916a7b872'), ObjectId('688e8dbbb4debce916a7b873'), ObjectId('688e8dbbb4debce916a7b874'), ObjectId('688e8dbbb4debce916a7b875'), ObjectId('688e8dbbb4debce916a7b876'), ObjectId('688e8dbbb4debce916a7b877'), ObjectId('688e8dbbb4debce916a7b878'), ObjectId('688e8dbbb4debce916a7b879'), ObjectId('688e8dbbb4debce916a7b87a'), ObjectId('688e8dbbb4debce916a7b87b'), ObjectId('688e8dbbb4debce916a7b87c'), ObjectId('688e8dbbb4debce916a7b87d'), ObjectId('688e8dbbb4debce916a7b87e'), ObjectId('688e8dbbb4debce916a7b87f'), ObjectId('688e8dbbb4debce916a7b880'), ObjectId('688e8dbbb4debce916a7b881'), ObjectId('688e8dbbb4debce916a7b882'), ObjectId('688e8dbbb4debce916a7b883'), ObjectId('688e8dbbb4debce916a7b884'), ObjectId('688e8dbbb4debce916a7b885'), ObjectId('688e8dbbb4debce916a7b886'), ObjectId('688e8dbbb4debce916a7b887'), ObjectId('688e8dbbb4debce916a7b888'), ObjectId('688e8dbbb4debce916a7b889'), ObjectId('688e8dbbb4debce916a7b88a'), ObjectId('688e8dbbb4debce916a7b88b'), ObjectId('688e8dbbb4debce916a7b88c'), ObjectId('688e8dbbb4debce916a7b88d'), ObjectId('688e8dbbb4debce916a7b88e'), ObjectId('688e8dbbb4debce916a7b88f'), ObjectId('688e8dbbb4debce916a7b890'), ObjectId('688e8dbbb4debce916a7b891'), ObjectId('688e8dbbb4debce916a7b892'), ObjectId('688e8dbbb4debce916a7b893'), ObjectId('688e8dbbb4debce916a7b894'), ObjectId('688e8dbbb4debce916a7b895'), ObjectId('688e8dbbb4debce916a7b896'), ObjectId('688e8dbbb4debce916a7b897'), ObjectId('688e8dbbb4debce916a7b898'), ObjectId('688e8dbbb4debce916a7b899'), ObjectId('688e8dbbb4debce916a7b89a'), ObjectId('688e8dbbb4debce916a7b89b'), ObjectId('688e8dbbb4debce916a7b89c'), ObjectId('688e8dbbb4debce916a7b89d'), ObjectId('688e8dbbb4debce916a7b89e'), ObjectId('688e8dbbb4debce916a7b89f'), ObjectId('688e8dbbb4debce916a7b8a0'), ObjectId('688e8dbbb4debce916a7b8a1'), ObjectId('688e8dbbb4debce916a7b8a2'), ObjectId('688e8dbbb4debce916a7b8a3'), ObjectId('688e8dbbb4debce916a7b8a4'), ObjectId('688e8dbbb4debce916a7b8a5')], acknowledged=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# The Sports Today\n",
    "with open(\"eventos_2025_sportstoday.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_sportstoday = json.load(f)\n",
    "    \n",
    "# Exportar los datos a una coleccion en mongoCompass llamada \"sports\"\n",
    "db[\"sports\"].insert_many(data_sportstoday)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab8b811a-e582-4f10-84a3-b201ae8d7e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InsertManyResult([ObjectId('688e8dbeb4debce916a7b9a0'), ObjectId('688e8dbeb4debce916a7b9a1'), ObjectId('688e8dbeb4debce916a7b9a2'), ObjectId('688e8dbeb4debce916a7b9a3'), ObjectId('688e8dbeb4debce916a7b9a4'), ObjectId('688e8dbeb4debce916a7b9a5'), ObjectId('688e8dbeb4debce916a7b9a6'), ObjectId('688e8dbeb4debce916a7b9a7'), ObjectId('688e8dbeb4debce916a7b9a8'), ObjectId('688e8dbeb4debce916a7b9a9'), ObjectId('688e8dbeb4debce916a7b9aa'), ObjectId('688e8dbeb4debce916a7b9ab'), ObjectId('688e8dbeb4debce916a7b9ac'), ObjectId('688e8dbeb4debce916a7b9ad'), ObjectId('688e8dbeb4debce916a7b9ae'), ObjectId('688e8dbeb4debce916a7b9af'), ObjectId('688e8dbeb4debce916a7b9b0'), ObjectId('688e8dbeb4debce916a7b9b1')], acknowledged=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir archivos csv en df y exportarlos\n",
    "df1 = pd.read_csv(\"eventos_2025.csv\")\n",
    "df2 = pd.read_csv(\"eventos_mayo_2025.csv\")\n",
    "\n",
    "# Convertir DataFrame a diccionarios y subir a la coleccion \"sports\"\n",
    "db[\"sports\"].insert_many(df1.to_dict(orient=\"records\"))\n",
    "db[\"sports\"].insert_many(df2.to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6d89f97-ab26-4eab-ad1d-9bfe566216e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "RangeIndex(start=0, stop=0, step=1)\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "# Conexión a MongoDB\n",
    "client = MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"eventos_deportivos_2025\"]\n",
    "collection = db[\"eventos_2025\"]\n",
    "\n",
    "# Obtener todos los documentos\n",
    "datos = list(collection.find({}, {\"_id\": 0}))  # Excluir el campo _id\n",
    "\n",
    "# Crear DataFrame\n",
    "df = pd.DataFrame(datos)\n",
    "\n",
    "# Vista previa\n",
    "print(df.head())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bc04a970-fb76-43d2-bf31-a75a3e1da80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concierto y Eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "850f99af-0a3c-4e40-a8d4-e7b0c43fc750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardados 7 eventos de gira en CSV y JSON\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Lista que contiene artistas y las URLs correspondientes \n",
    "tours = [\n",
    "    {\"artist\": \"Beyoncé\", \"url\": \"https://en.wikipedia.org/wiki/Cowboy_Carter_Tour\"},\n",
    "    {\"artist\": \"Katy Perry\", \"url\": \"https://en.wikipedia.org/wiki/The_Lifetimes_Tour\"},\n",
    "    {\"artist\": \"Lainey Wilson\", \"url\": \"https://en.wikipedia.org/wiki/Whirlwind_World_Tour\"},\n",
    "    {\"artist\": \"Iron Maiden\", \"url\": \"https://en.wikipedia.org/wiki/Run_for_Your_Lives_World_Tour\"},\n",
    "    {\"artist\": \"Oasis\", \"url\": \"https://en.wikipedia.org/wiki/Oasis_Live_%2725_Tour\"},\n",
    "    {\"artist\": \"Blackpink\", \"url\": \"https://en.wikipedia.org/wiki/Blackpink_2025_World_Tour\"},\n",
    "    {\"artist\": \"Lady Gaga\", \"url\": \"https://en.wikipedia.org/wiki/The_Mayhem_Ball\"}\n",
    "]\n",
    "\n",
    "# Itera sobre cada elemento de la lista y lo analiza como HTML\n",
    "data = []\n",
    "for tour in tours:\n",
    "    resp = requests.get(tour[\"url\"])\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    # Extraemos fechas de inicio-fin y número de shows\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if infobox:\n",
    "        rows = infobox.find_all(\"tr\")\n",
    "        info = {\"artista\": tour[\"artist\"]}\n",
    "        for tr in rows:\n",
    "            th = tr.find(\"th\")\n",
    "            td = tr.find(\"td\")\n",
    "            if th and td:\n",
    "                key = th.text.strip().lower()  # Limpia el texto y convierte la clave a minúsculas para normalizar.\n",
    "                val = td.text.strip()\n",
    "                # Asocia los datos encontrados a las claves esperadas (inicio, fin, número de shows).\n",
    "                if \"start date\" in key:\n",
    "                    info[\"inicio\"] = val\n",
    "                elif \"end date\" in key:\n",
    "                    info[\"fin\"] = val\n",
    "                elif \"number of shows\" in key:\n",
    "                    info[\"n_shows\"] = val\n",
    "        data.append(info) # Guarda el diccionario en la lista de datos\n",
    "\n",
    "# Guardar en CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"conciertos_giras_2025.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"Guardados {len(data)} eventos de gira en CSV y JSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7b1c6523-8dc6-4654-af00-1efacccdffe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardados 3 registros en giras_conciertos_2025.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Lista que contiene artistas y las URLs correspondientes\n",
    "tours = [\n",
    "    {\"artist\": \"Oasis\", \"url\": \"https://en.wikipedia.org/wiki/Oasis_Live_%2725_Tour\"},\n",
    "    {\"artist\": \"Iron Maiden\", \"url\": \"https://en.wikipedia.org/wiki/Run_for_Your_Lives_World_Tour\"},\n",
    "    {\"artist\": \"Kendrick Lamar & SZA\", \"url\": \"https://en.wikipedia.org/wiki/Grand_National_Tour\"}\n",
    "]\n",
    "\n",
    "# Itera sobre cada elemento de la lista y lo analiza como HTML\n",
    "data = []\n",
    "for tour in tours:\n",
    "    resp = requests.get(tour[\"url\"])\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    info = {\"artista\": tour[\"artist\"]}\n",
    "    # Buscar tabla infobox\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    if infobox:\n",
    "        for tr in infobox.find_all(\"tr\"):\n",
    "            th = tr.find(\"th\"); td = tr.find(\"td\")\n",
    "            if not (th and td): continue\n",
    "            key = th.get_text(strip=True).lower()\n",
    "            val = td.get_text(\" \", strip=True)\n",
    "            if \"start date\" in key:\n",
    "                info[\"inicio\"] = val\n",
    "            elif \"end date\" in key:\n",
    "                info[\"fin\"] = val\n",
    "            elif \"number of shows\" in key:\n",
    "                info[\"n_shows\"] = val\n",
    "    data.append(info)\n",
    "\n",
    "# Guardar como JSON\n",
    "with open(\"giras_conciertos_2025.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Guardados {len(data)} registros en giras_conciertos_2025.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "974e1c3f-7ce2-437b-87dd-3c72b523ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos guardados en 'giras_conciertos_2025.txt'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Giras y sus URLs\n",
    "tours = [\n",
    "    {\"artist\": \"Beyoncé\", \"url\": \"https://en.wikipedia.org/wiki/Cowboy_Carter_Tour\"},\n",
    "    {\"artist\": \"Blackpink\", \"url\": \"https://en.wikipedia.org/wiki/Deadline_World_Tour\"}\n",
    "]\n",
    "\n",
    "# Lista para almacenar líneas de texto\n",
    "lines = []\n",
    "\n",
    "for tour in tours:\n",
    "    resp = requests.get(tour[\"url\"])\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    \n",
    "    # Extraer datos clave\n",
    "    data = {\"artista\": tour[\"artist\"]}\n",
    "    infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "    \n",
    "    if infobox:\n",
    "        for tr in infobox.find_all(\"tr\"):\n",
    "            th = tr.find(\"th\")\n",
    "            td = tr.find(\"td\")\n",
    "            if not (th and td): continue\n",
    "            key = th.get_text(strip=True).lower()\n",
    "            val = td.get_text(\" \", strip=True)\n",
    "            if \"concert tour name\" in key:\n",
    "                data[\"nombre_tour\"] = val\n",
    "            elif \"start date\" in key:\n",
    "                data[\"inicio\"] = val\n",
    "            elif \"end date\" in key:\n",
    "                data[\"fin\"] = val\n",
    "            elif \"number of shows\" in key:\n",
    "                data[\"n_shows\"] = val\n",
    "\n",
    "    # Formatear línea de texto\n",
    "    lines.append(f\"Artista: {data.get('artista', 'N/A')}\")\n",
    "    lines.append(f\"Nombre del Tour: {data.get('nombre_tour', 'N/A')}\")\n",
    "    lines.append(f\"Inicio: {data.get('inicio', 'N/A')}\")\n",
    "    lines.append(f\"Fin: {data.get('fin', 'N/A')}\")\n",
    "    lines.append(f\"Número de Shows: {data.get('n_shows', 'N/A')}\")\n",
    "    lines.append(\"-\" * 40)\n",
    "\n",
    "# Guardar en archivo .txt\n",
    "with open(\"giras_conciertos_2025.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "print(\"Datos guardados en 'giras_conciertos_2025.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1fc7c7f-bd4a-42b2-b414-ad858c15f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Festivales de Musica\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bdf54889-4a85-479b-85c1-1a99f694f956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardados 24 festivales en 'festivales_2025_scraping.json'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Fuentes web seleccionadas\n",
    "urls = [\n",
    "    \"https://www.musiclipse.com/2025/01/20/top-10-music-festivals-around-the-world-to-attend-in-2025\",\n",
    "    \"https://wavytrip.com/2025/01/13/top-2025-music-festivals\"\n",
    "]\n",
    "\n",
    "festivales = []\n",
    "\n",
    "for url in urls:\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "    # Buscar encabezados y párrafos con información relevante\n",
    "    for section in soup.find_all(['h2','h3','p']):\n",
    "        text = section.get_text(strip=True)\n",
    "        # Detectamos entradas típicas con formato de nombre + fecha + ubicación\n",
    "        if any(month in text for month in [\"2025\",\"June\",\"July\",\"August\",\"April\",\"May\"]) and text.count(',')>=1:\n",
    "            festivales.append({\"info\": text, \"fuente\": url})\n",
    "\n",
    "# Procesar las entradas\n",
    "datos = []\n",
    "for item in festivales:\n",
    "    raw = item[\"info\"]\n",
    "    origen = item[\"fuente\"]\n",
    "    # Intentamos separar nombre, fechas y ubicación\n",
    "    parts = raw.split(\"Date\") if \"Date\" in raw else raw.split(\"Date:\")\n",
    "    if len(parts) > 1:\n",
    "        nombre = parts[0].strip()\n",
    "        rest = parts[1]\n",
    "        rest = rest.replace(\"Location:\", \"|\")\n",
    "        rest_parts = rest.split(\"|\")\n",
    "        fechas = rest_parts[0].strip()\n",
    "        ubicacion = rest_parts[1].strip() if len(rest_parts)>1 else \"\"\n",
    "    else:\n",
    "        comps = raw.split(\",\")\n",
    "        nombre = comps[0].strip()\n",
    "        fechas = comps[1].strip() if len(comps)>1 else \"\"\n",
    "        ubicacion = comps[-1].strip()\n",
    "    datos.append({\n",
    "        \"nombre\": nombre,\n",
    "        \"fechas\": fechas,\n",
    "        \"ubicacion\": ubicacion,\n",
    "        \"fuente\": origen\n",
    "    })\n",
    "\n",
    "# Guardar archivo JSON\n",
    "with open(\"festivales_2025_scraping.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(datos, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Guardados {len(datos)} festivales en 'festivales_2025_scraping.json'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e7cd87c-4b6c-4914-a9f8-90c2cb9ce2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraídos 10 restaurantes desde Casa Gangotena.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "url = \"https://www.casagangotena.com/es/blog/consejos-de-viaje/mejores-restaurantes-centro-historico-quito/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# En esta página, cada restaurante aparece dentro de un encabezado <h3> seguido de un párrafo\n",
    "items = soup.find_all(\"h3\")\n",
    "\n",
    "restaurantes = []\n",
    "for h3 in items:\n",
    "    nombre = h3.get_text(strip=True)\n",
    "    # el párrafo siguiente suele contener la descripción\n",
    "    descripcion = \"\"\n",
    "    if h3.find_next_sibling(\"p\"):\n",
    "        descripcion = h3.find_next_sibling(\"p\").get_text(strip=True)\n",
    "    restaurantes.append({\"nombre\": nombre, \"descripcion\": descripcion})\n",
    "\n",
    "# Guardar como CSV\n",
    "with open(\"top10_restaurantes_quito.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"nombre\", \"descripcion\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(restaurantes)\n",
    "\n",
    "print(f\"Extraídos {len(restaurantes)} restaurantes desde Casa Gangotena.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2eb8802e-d8cf-48b7-a02d-f826e82c1a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "# Conexion con sqlite en la db \"proyecto_analisis\"\n",
    "conn = sqlite3.connect(\"proyecto_analisis.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS conciertos (artista TEXT, inicio text, fin TEXT)\")\n",
    "\n",
    "# Exportar los datos del csv a una tabla \"conciertos\" en sqlite\n",
    "with open(\"conciertos_giras_2025.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        cursor.execute(\"INSERT INTO conciertos (artista, inicio, fin) VALUES (?,?,?)\", row)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32253808-0792-44ac-8d14-09754271da40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "# Conexion con sqlite en la db \"proyecto_analisis\"\n",
    "conn = sqlite3.connect(\"proyecto_analisis.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS giras (artista TEXT, inicio TEXT,fin text)\")\n",
    "\n",
    "# Exportar los datos del json a una tabla \"giras\" en sqlite\n",
    "with open(\"giras_conciertos_2025.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for row in data:\n",
    "        cursor.execute(\"INSERT INTO giras (artista, inicio, fin) VALUES (?,?, ?)\", (row[\"artista\"],row[\"inicio\"],row[\"fin\"]))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e21c13e-a527-44a7-9750-0cf4e4d6d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "import json\n",
    "\n",
    "# Conexion con sqlite en la db \"proyecto_analisis\"\n",
    "conn = sqlite3.connect(\"proyecto_analisis.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS festival (nombre TEXT, fechas TEXT,ubicacion text)\")\n",
    "\n",
    "# Exportar los datos del json a una tabla \"festival\" en sqlite\n",
    "with open(\"festivales_2025_scraping.json\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    for row in data:\n",
    "        cursor.execute(\"INSERT INTO festival (nombre, fechas, ubicacion) VALUES (?,?, ?)\", (row[\"nombre\"],row[\"fechas\"],row[\"ubicacion\"]))\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48be2e1c-7c1f-445b-a0dc-a99cb7fba4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import csv\n",
    "\n",
    "# Conexion con sqlite en la db \"proyecto_analisis\"\n",
    "conn = sqlite3.connect(\"proyecto_analisis.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"CREATE TABLE IF NOT EXISTS restaurantes (nombre TEXT, descripcion text)\")\n",
    "\n",
    "# Exportar los datos del csv a una tabla \"restaurantes\" en sqlite\n",
    "with open(\"top10_restaurantes_quito.csv\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        cursor.execute(\"INSERT INTO restaurantes (nombre,descripcion) VALUES (?,?)\", row)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e33f0738-cbea-4e56-91c3-e50ed7be5d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACTIVIDADES Y HOBBIES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f8c628-e0f8-40c7-ba4a-4d18e00e69d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 153 actividades guardadas en actividades_quito.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.proturec.com/quito/actividades-quito/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "res = requests.get(url, headers=headers)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "actividades = []\n",
    "\n",
    "# Ejemplo: extraer párrafos con bullet points\n",
    "for ul in soup.find_all(\"ul\"):\n",
    "    for li in ul.find_all(\"li\"):\n",
    "        texto = li.get_text(strip=True)\n",
    "        if texto:\n",
    "            actividades.append({\"actividad\": texto})\n",
    "\n",
    "# Eliminar duplicados\n",
    "actividades = [dict(t) for t in {tuple(d.items()) for d in actividades}]\n",
    "\n",
    "# Guardar en JSON\n",
    "with open(\"actividades_quito.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(actividades, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ {len(actividades)} actividades guardadas en actividades_quito.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e24abee6-ccdd-47f1-acb3-b64ca77910d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 42 hobbies guardados en hobbies_wikihow.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://www.wikihow.com/Find-Hobbies\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "hobbies = []\n",
    "\n",
    "# Extraer las listas de hobbies de la página (dentro de secciones <ul>)\n",
    "for ul in soup.select(\"ul\"):\n",
    "    for li in ul.find_all(\"li\"):\n",
    "        texto = li.get_text(strip=True)\n",
    "        if texto and len(texto) < 80:  # Filtrar textos muy largos\n",
    "            hobbies.append(texto)\n",
    "\n",
    "# Eliminar duplicados\n",
    "hobbies = list(dict.fromkeys(hobbies))\n",
    "\n",
    "with open(\"hobbies_wikihow.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hobbies, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ {len(hobbies)} hobbies guardados en hobbies_wikihow.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20bfb890-675c-4068-b197-599b74db661b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 23 eventos guardados en 'eventos_ecuador.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = \"https://www.timeanddate.com/holidays/ecuador/\"\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# Realizar solicitud HTTP y verificar estado\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "\n",
    "# Analizar el HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Lista para almacenar los eventos extraídos\n",
    "eventos = []\n",
    "\n",
    "# Buscar la tabla principal de días festivos\n",
    "tabla = soup.find(\"table\", attrs={\"id\": \"holidays-table\"})\n",
    "if tabla:\n",
    "    # Recorrer cada fila (excepto el encabezado)\n",
    "    for fila in tabla.find_all(\"tr\")[1:]:\n",
    "        columnas = fila.find_all(\"td\")\n",
    "        if len(columnas) >= 3:\n",
    "            fecha = columnas[0].text.strip()\n",
    "            nombre = columnas[1].text.strip()\n",
    "            tipo = columnas[2].text.strip()\n",
    "            eventos.append([fecha, nombre, tipo])\n",
    "\n",
    "# Guardar los datos en un archivo CSV\n",
    "with open(\"eventos_ecuador.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Fecha\", \"Nombre\", \"Tipo\"])  # Cabecera\n",
    "    writer.writerows(eventos)\n",
    "\n",
    "print(f\"✅ {len(eventos)} eventos guardados en 'eventos_ecuador.csv'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30fe5f74-bee0-4487-99c7-4cb0fb2bccab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Insertados 23 eventos desde eventos_ecuador.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# URI de conexión a MongoDB local\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "\n",
    "def insertar_eventos_csv(ruta):\n",
    "    try:\n",
    "        # Conexión con MongoDB\n",
    "        client = MongoClient(MONGO_URI)\n",
    "        db = client[\"eventos_deportivos_2025\"]\n",
    "        collection = db[\"actividades_eventos\"]\n",
    "\n",
    "        eventos = []\n",
    "        # Leer archivo CSV y procesar cada fila\n",
    "        with open(ruta, encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                eventos.append({\n",
    "                    \"tipo_dato\": \"evento\",  # Etiqueta útil para filtrado en la colección\n",
    "                    \"fecha\": row.get(\"Fecha\"),\n",
    "                    \"nombre\": row.get(\"Nombre\"),\n",
    "                    \"categoria\": row.get(\"Tipo\")\n",
    "                })\n",
    "\n",
    "        # Insertar documentos si hay eventos válidos\n",
    "        if eventos:\n",
    "            result = collection.insert_many(eventos)\n",
    "            print(f\"✅ Insertados {len(result.inserted_ids)} eventos desde {ruta}\")\n",
    "\n",
    "        # Cerrar conexión\n",
    "        client.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al insertar eventos desde {ruta}: {e}\")\n",
    "\n",
    "# Ejecución directa\n",
    "if __name__ == \"__main__\":\n",
    "    insertar_eventos_csv(\"eventos_ecuador.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea6a6730-1e93-469b-ad4f-7bbc7237a35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Insertados 42 hobbies desde hobbies_wikihow.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "\n",
    "def insertar_hobbies_json(ruta):\n",
    "    try: # Crea conexión a la base eventos_deportivos_2025 y accede (o crea) la colección actividades_eventos\n",
    "        client = MongoClient(MONGO_URI)\n",
    "        db = client[\"eventos_deportivos_2025\"]\n",
    "        collection = db[\"actividades_eventos\"]\n",
    "        \n",
    "# Lee el archivo JSON y genera una lista de diccionarios, uno por hobby, con una etiqueta de tipo de dato\n",
    "        with open(ruta, encoding=\"utf-8\") as f:\n",
    "            hobbies = json.load(f)\n",
    "            hobbies_docs = [{\"tipo_dato\": \"hobby\", \"nombre\": h} for h in hobbies]\n",
    "        # Inserta todos los documentos en MongoDB y muestra confirmación con el número de inserciones realizadas\n",
    "        if hobbies_docs: \n",
    "            result = collection.insert_many(hobbies_docs)\n",
    "            print(f\"✅ Insertados {len(result.inserted_ids)} hobbies desde {ruta}\")\n",
    "        client.close()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al insertar hobbies desde {ruta}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    insertar_hobbies_json(\"hobbies_wikihow.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17a99b28-9342-4683-bf38-631fe483abe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Insertadas 153 actividades desde actividades_quito.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017/\"\n",
    "\n",
    "def insertar_actividades_json(ruta):\n",
    "    try:\n",
    "        client = MongoClient(MONGO_URI)\n",
    "        db = client[\"eventos_deportivos_2025\"]\n",
    "        collection = db[\"actividades_eventos\"]\n",
    "\n",
    "        with open(ruta, encoding=\"utf-8\") as f:\n",
    "            actividades = json.load(f)\n",
    "            actividades_docs = []\n",
    "            for a in actividades:\n",
    "                doc = {\"tipo_dato\": \"actividad\"}\n",
    "                doc.update(a)\n",
    "                actividades_docs.append(doc)\n",
    "        if actividades_docs:\n",
    "            result = collection.insert_many(actividades_docs)\n",
    "            print(f\"✅ Insertadas {len(result.inserted_ids)} actividades desde {ruta}\")\n",
    "        client.close()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al insertar actividades desde {ruta}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    insertar_actividades_json(\"actividades_quito.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3dbe5-69aa-46ba-a6c2-7da8a2ca251d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Configura las cadenas de conexión\n",
    "LOCAL_URI = \"mongodb://localhost:27017/\"\n",
    "ATLAS_URI = \"mongodb+srv://kevin:jony12344@cluster0.sfbgrnk.mongodb.net/\"\n",
    "\n",
    "DB_NAME = \"eventos_deportivos_2025\"\n",
    "COLLECTION_NAME = \"sports\"\n",
    "\n",
    "# Conectar a MongoDB local\n",
    "client_local = MongoClient(LOCAL_URI)\n",
    "db_local = client_local[DB_NAME]\n",
    "colec_local = db_local[COLLECTION_NAME]\n",
    "\n",
    "# Conectar a MongoDB Atlas\n",
    "client_atlas = MongoClient(ATLAS_URI)\n",
    "db_atlas = client_atlas[DB_NAME]\n",
    "colec_atlas = db_atlas[COLLECTION_NAME]\n",
    "\n",
    "# Obtener todos los documentos de la colección local\n",
    "documentos = list(colec_local.find())\n",
    "\n",
    "if not documentos:\n",
    "    print(\"❌ No se encontraron documentos en la colección local\")\n",
    "else:\n",
    "    # Quitar _id para evitar conflicto en Atlas\n",
    "    for doc in documentos:\n",
    "        if \"_id\" in doc:\n",
    "            del doc[\"_id\"]\n",
    "\n",
    "    # Insertar documentos en Atlas\n",
    "    result = colec_atlas.insert_many(documentos)\n",
    "    print(f\"✅ Migrados {len(result.inserted_ids)} documentos a MongoDB Atlas\")\n",
    "\n",
    "# Cerrar conexiones\n",
    "client_local.close()\n",
    "client_atlas.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01c17b97-357d-45ee-a4f4-ce7f10501274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Migrados 330 documentos de 'nombre_db_local.nombre_coleccion_local' a 'nombre_db_atlas.nombre_coleccion_atlas'\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "# Migracion de datos de mongoDB a Atlas\n",
    "LOCAL_URI = \"mongodb://localhost:27017/\"\n",
    "ATLAS_URI = \"mongodb+srv://kevin:jony12344@cluster0.sfbgrnk.mongodb.net/\"\n",
    "\n",
    "local_client = MongoClient(LOCAL_URI)\n",
    "atlas_client = MongoClient(ATLAS_URI)\n",
    "\n",
    "local_col = local_client[\"eventos_deportivos_2025\"][\"sports\"]\n",
    "atlas_col = atlas_client[\"proyecto_analisis\"][\"parte1\"]\n",
    "\n",
    "docs = [{k: v for k, v in doc.items() if k != \"_id\"} for doc in local_col.find()]\n",
    "\n",
    "if docs:\n",
    "    result = atlas_col.insert_many(docs)\n",
    "    print(f\"✅ Migrados {len(result.inserted_ids)} documentos de 'nombre_db_local.nombre_coleccion_local' a 'nombre_db_atlas.nombre_coleccion_atlas'\")\n",
    "else:\n",
    "    print(\"⚠️ No se encontraron documentos para migrar.\")\n",
    "\n",
    "local_client.close()\n",
    "atlas_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "411f1b99-e9d8-4611-bfa3-bec32e684cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Migrados 218 documentos de 'nombre_db_local.nombre_coleccion_local' a 'nombre_db_atlas.nombre_coleccion_atlas'\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "LOCAL_URI = \"mongodb://localhost:27017/\"\n",
    "ATLAS_URI = \"mongodb+srv://kevin:jony12344@cluster0.sfbgrnk.mongodb.net/\"\n",
    "\n",
    "local_client = MongoClient(LOCAL_URI)\n",
    "atlas_client = MongoClient(ATLAS_URI)\n",
    "\n",
    "local_col = local_client[\"eventos_deportivos_2025\"][\"actividades_eventos\"]\n",
    "atlas_col = atlas_client[\"proyecto_analisis\"][\"parte1\"]\n",
    "\n",
    "docs = [{k: v for k, v in doc.items() if k != \"_id\"} for doc in local_col.find()]\n",
    "\n",
    "if docs:\n",
    "    result = atlas_col.insert_many(docs)\n",
    "    print(f\"✅ Migrados {len(result.inserted_ids)} documentos de 'nombre_db_local.nombre_coleccion_local' a 'nombre_db_atlas.nombre_coleccion_atlas'\")\n",
    "else:\n",
    "    print(\"⚠️ No se encontraron documentos para migrar.\")\n",
    "\n",
    "local_client.close()\n",
    "atlas_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c41d327c-c90c-42e1-a918-a34088f70d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Insertados 46 documentos en la colección 'parte1' de la base 'proyecto_analisis'\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Migracion de datos de SQLite a Atlas\n",
    "SQLITE_PATH = \"proyecto_analisis.db\"\n",
    "ATLAS_URI = \"mongodb+srv://kevin:jony12344@cluster0.sfbgrnk.mongodb.net/\"\n",
    "MONGO_DB_NAME = \"proyecto_analisis\"\n",
    "MONGO_COLLECTION_NAME = \"parte1\"\n",
    "\n",
    "# Conexión SQLite\n",
    "conn = sqlite3.connect(SQLITE_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Conexión Mongo Atlas\n",
    "client = MongoClient(ATLAS_URI)\n",
    "db = client[MONGO_DB_NAME]\n",
    "collection = db[MONGO_COLLECTION_NAME]\n",
    "\n",
    "# Obtener todas las tablas de SQLite\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tablas = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "documentos_totales = []\n",
    "\n",
    "for tabla in tablas:\n",
    "    cursor.execute(f\"SELECT * FROM {tabla}\")\n",
    "    columnas = [desc[0] for desc in cursor.description]\n",
    "    filas = cursor.fetchall()\n",
    "\n",
    "    for fila in filas:\n",
    "        doc = {\"tabla\": tabla}  # Para saber de qué tabla viene el dato\n",
    "        for col, val in zip(columnas, fila):\n",
    "            doc[col] = val\n",
    "        documentos_totales.append(doc)\n",
    "\n",
    "if documentos_totales:\n",
    "    collection.insert_many(documentos_totales)\n",
    "    print(f\"✅ Insertados {len(documentos_totales)} documentos en la colección '{MONGO_COLLECTION_NAME}' de la base '{MONGO_DB_NAME}'\")\n",
    "\n",
    "conn.close()\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2461a01-ba83-4c71-bba7-8ea6a20ce122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
